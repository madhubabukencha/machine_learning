{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be147d61",
   "metadata": {},
   "source": [
    "<p style=\"color:#153462; \n",
    "          font-weight: bold; \n",
    "          font-size: 30px; \n",
    "          font-family: Gill Sans, sans-serif; \n",
    "          text-align: center;\">\n",
    "          Decision Tree Theory</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e25af",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "\tDecision Trees are versatile Machine Learning algorithms that can perform both classification and regression tasks and even multioutput tasks. They are powerful algorithms caple of fitting complex datasets. Using the decision algorithm, we start at the tree root node and split the data on the feature that results in the largest <b><i>information gain(IG)</i></b>. In practice, this can result in a very deep tree with many nodes, which can esily lead to overfitting. Thus we typically want to <b><i>prune</i></b> the tree by setting a limit for the maximum depth of the tree.\n",
    "   </font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45904f6a",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "    To split the nodes at the most informative features, we need an objective function to optimize via the tree learning algorithm. Below is the function to maximize the IG at each split: <br>\n",
    "       $$IG(D_p, f) = I(D_p) - \\sum_{j=1}^{m} \\frac{N_j}{N_p}I(D_j)$$\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abcf0aa",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "\tHere, <br>\n",
    "    f is the feature to perform split<br>\n",
    "    $D_{p}$ and $D_{j}$ are the dataset of the parent and jth child node<br>\n",
    "    $I$ is our impuity measure<br>\n",
    "    $N_{p}$ is the total number of training examples at the parent node<br>\n",
    "    $N_{j}$ is the number of example in the jth node\n",
    "   </font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b97a33",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "    Most of the libraries implement binary decision trees. This means that each parent node is split into two child nodes, $D_{left}$ and $D_{right}$. So the above equation can be written as,<br>\n",
    "       $$IG(D_{p}, f) = I(D_{p}) - \\frac{N_{j\\_left}}{N_{p}}I(D_{left})- \\frac{N_{j\\_right}}{N_{p}}I(D_{right})$$\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8bc4f3",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "    We have 3 impurity measures or splitting criteria that are commonly used in binary decision tree algoirthm.\n",
    "    <ul>\n",
    "        <li>\n",
    "            Gini Impurity:(This the commonly used impurity mesure in decision trees)<br>\n",
    "            $I_{G}(t) = 1-\\displaystyle\\sum_{i=1}^{c}p(i|t)^2$\n",
    "        </li>\n",
    "        <li>\n",
    "            Entropy: <br>\n",
    "            $I_{H}(t) = -\\displaystyle\\sum_{i=1} ^{c}p(i|t)log_{2} p(i|t)$\n",
    "        </li>\n",
    "        <li>\n",
    "            Classification Error:<br>\n",
    "            $I_{E}(t) = 1 - max\\{p(i|t)\\}$\n",
    "        </li>\n",
    "    </ul>\n",
    "    Here, p(i|t) is the proportion of the examples that belong to class i for a particular node, t.\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b407e0f9",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "    We will understand above formulas with some calculations. To do so, I'm using 2 simple decision tree graphs.\n",
    "    <img src=\"images/sample_tree.png\" alt=\"sample_tree\" style=\"width: 600px;\"/>\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452368c4",
   "metadata": {},
   "source": [
    "<p style=\"color:#3C4048; \n",
    "          font-weight: bold; \n",
    "          font-size: 18px; \n",
    "          font-family: Gill Sans, sans-serif;\">\n",
    "          Calculating IG with Classification Error:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb568b98",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       We will identify in <code>Tree-A</code> or <code>Tree-B </code>, what will best tree to consider for spliting using <code>classification error</code>. <br>\n",
    "       calculating impurity at root node. This is same for trees.<br><br>\n",
    "       $I_{E} (D_{p}) = 1 - max\\{\\frac{40}{80}, \\frac{40}{80}\\} = 1 - \\frac{40}{80} = 1- \\frac{1}{2} = 0.5$\n",
    "        <hr style=\"border: 1px solid red; width:50%;\">\n",
    "        $Tree-A: I_{E} (D_{left}) = 1 -max\\{\\frac{30}{40}, \\frac{10}{40}\\} = 1 - \\frac{30}{40} = 1 -  \\frac{3}{4} = 0.25$ <br><br>\n",
    "       $Tree-A: I_{E} (D_{right}) = 1 -max\\{\\frac{10}{40}, \\frac{30}{40}\\} = 1 - \\frac{30}{40} = 1 - \\frac{3}{4} = 0.25$ <br><br>\n",
    "       Information Gain at <code>Tree-A</code>: <br><br>                                              \n",
    "       $IG_{E} = I(D_{p}) - \\frac{N_{left}}{N_{p}}I(D_{left})- \\frac{N_{right}}{N_{p}}I(D_{right}) = 0.5 - \\frac{40}{80} * 0.25 - \\frac{40}{80} * 0.25 = 0.25$\n",
    "       <hr style=\"border: 1px solid red; width:50%\">\n",
    "        $Tree-B: I_{E} (D_{left}) = 1 -max\\{\\frac{20}{60}, \\frac{40}{60}\\} = 1 - \\frac{4}{6} = \\frac{2}{6} = \\frac{1}{3}$ <br><br>\n",
    "       $Tree-B: I_{E} (D_{right}) = 1 -max\\{\\frac{20}{20}, \\frac{0}{40}\\} = 1 - \\frac{20}{20} = 1 - 1 = 0$ <br><br>\n",
    "       Information Gain at <code>Tree-B</code>: <br><br>\n",
    "       $IG_{E} = I(D_{p}) - \\frac{N_{left}}{N_{p}}I(D_{left})- \\frac{N_{right}}{N_{p}}I(D_{right}) = 0.5 - \\frac{60}{80} * \\frac{1}{3} - \\frac{20}{80} * 0 = 0.25$<br><br>\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e98aa",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       When we used classification error as a spliting criterion we saw no difference in Information Gain(IG) for both Tree-A and Tree-B \n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e596a8ae",
   "metadata": {},
   "source": [
    "<p style=\"color:#3C4048; \n",
    "          font-weight: bold; \n",
    "          font-size: 18px; \n",
    "          font-family: Gill Sans, sans-serif;\">\n",
    "          Calculating IG with Gini Impurity Measure:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4515591",
   "metadata": {},
   "source": [
    "$I_{G}(D_{p}) = 1 - \\left(\\left(\\frac{40}{80}\\right)^{2} + \\left(\\frac{40}{80}\\right)^2\\right) = 0.5$\n",
    "<hr style=\"border: 1px solid red; width:50%;\">\n",
    "<strong>At Tree-A</strong>: <br>\n",
    "$I_{G}(D_{left}) = 1 - \\left(\\left(\\frac{3}{4}\\right)^{2} + \\left(\\frac{1}{4}\\right)^2\\right) = \\frac{3}{8} = 0.375$<br>\n",
    "$I_{G}(D_{right}) = 1 - \\left(\\left(\\frac{1}{4}\\right)^{2} + \\left(\\frac{3}{4}\\right)^2\\right) = \\frac{3}{8} = 0.375$<br>\n",
    "Information Gain at <code>Tree-A</code><br>\n",
    "$IG_{G}= 0.5 - \\frac{4}{8} * 0.375 - \\frac{4}{8} * 0.375 = 0.125$\n",
    "<hr style=\"border: 1px solid red; width:50%;\">\n",
    "<strong>At Tree-B</strong>: <br>\n",
    "$I_{G}(D_{left}) = 1 - \\left(\\left(\\frac{2}{6}\\right)^{2} + \\left(\\frac{4}{6}\\right)^2\\right) = \\frac{4}{9} = 0.444$<br>\n",
    "$I_{G}(D_{right}) = 1 - \\left(\\left(\\frac{2}{2}\\right)^{2} + \\left(\\frac{0}{2}\\right)^2\\right) = 0$<br>\n",
    "Information Gain at <code>Tree-B</code><br>\n",
    "$IG_{G}= 0.5 - \\frac{6}{8} * 0.444 - \\frac{2}{8} * 0 = 0.1666$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec4f6a3",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "    Since we have good information gain at Tree - B, we consider it is a good split\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec3a47b",
   "metadata": {},
   "source": [
    "<p style=\"color:#3C4048; \n",
    "          font-weight: bold; \n",
    "          font-size: 18px; \n",
    "          font-family: Gill Sans, sans-serif;\">\n",
    "          Calculating IG with Entropy Impurity Measure:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad59357",
   "metadata": {},
   "source": [
    "$I_{H}(D_{p}) = -(\\frac{40}{80} log_{2} \\left(\\frac{40}{80}\\right) + \\frac{40}{80}log_{2} \\left(\\frac{40}{80}\\right)) = 1$\n",
    "<br><br>\n",
    "<hr style=\"border: 1px solid red; width:50%;\">\n",
    "<b>At Tree-A</b><br>\n",
    "$I_{H}(D_{left}) = -(\\frac{30}{40} log_{2} \\left(\\frac{30}{40}\\right) + \\frac{10}{40}log_{2} \\left(\\frac{10}{40}\\right)) = 0.81$<br><br>\n",
    "$I_{H}(D_{right}) = -(\\frac{10}{40} log_{2} \\left(\\frac{10}{40}\\right) + \\frac{30}{40}log_{2} \\left(\\frac{30}{40}\\right)) = 0.81$<br><br>\n",
    "Information Gain at <code>Tree-A</code><br><br>\n",
    "$IG_{H}= 1 - \\frac{4}{8} * 0.81- \\frac{4}{8} * 0.81 = 0.19$<br><br>\n",
    "<hr style=\"border: 1px solid red; width:50%;\">\n",
    "<b>At Tree-B</b><br>\n",
    "$I_{H}(D_{left}) = -(\\frac{20}{60} log_{2} \\left(\\frac{20}{60}\\right) + \\frac{40}{60}log_{2} \\left(\\frac{40}{60}\\right)) = 0.92$<br><br>\n",
    "$I_{H}(D_{right}) =0$<br><br>\n",
    "Information Gain at <code>Tree-B</code><br><br>\n",
    "$IG_{H}= 1 - \\frac{6}{8} * 0.92- 0 = 0.31$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e018cd3",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "    Since we have good information gain at Tree - B, we consider it is a good split\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3f347d",
   "metadata": {},
   "source": [
    "<p style=\"color:#3C4048; \n",
    "          font-weight: bold; \n",
    "          font-size: 18px; \n",
    "          font-family: Gill Sans, sans-serif;\">\n",
    "          Regularization Hyperparameters:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ff3757",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "    To avoid overfitting of our training data, we need to restrict the Decision Tree free during training. This called Regularization. The regularization hyperparameters depend on the algorithm used.<br>\n",
    "       Below are some of important hyperparameters of Decision Tree in scikit learn:\n",
    "       <ul>\n",
    "           <li><i>max_depth</i>: Defines maximum depth of decision tree</li>\n",
    "           <li><i>min_samples_split</i>: The minimum number of samples a node must have before it can be split</li>\n",
    "           <li><i>min_samples_leaf</i>: The minimum number of sample a leaf node must have</li>\n",
    "           <li><i>max_leaf_nodes</i>: The maximum number of leaf nodes</li>\n",
    "           <li><i>max_features</i>: The maximum number of features that are evaluated for splitting at each node</li>\n",
    "       </ul>\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0499789",
   "metadata": {},
   "source": [
    "<p style=\"color:#3C4048; \n",
    "          font-weight: bold; \n",
    "          font-size: 18px; \n",
    "          font-family: Gill Sans, sans-serif;\">\n",
    "          Useful Resources:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f0dbda",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>\n",
    "        Implemented from scratch in Python:<br>\n",
    "        <a href=\"https://towardsdatascience.com/decision-tree-algorithm-in-python-from-scratch-8c43f0e40173\">https://towardsdatascience.com/decision-tree-algorithm-in-python-from-scratch-8c43f0e40173</a>\n",
    "    </li>\n",
    "    <li>\n",
    "        Scikit-Learn documentation, Contains mathematical formulation too:<br>\n",
    "        <a href=\"https://scikit-learn.org/stable/modules/tree.html\">https://scikit-learn.org/stable/modules/tree.html</a>\n",
    "    </li>\n",
    "</ul>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
